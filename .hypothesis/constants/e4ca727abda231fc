# file: C:\RAG\Qdrant\src\evaluation_engine\ragas_engine.py
# hypothesis_version: 6.148.7

[0.0, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 100, 1000, '.', 'A', 'B', 'C', 'D', 'F', 'OPENAI_API_KEY', 'actual', 'answer', 'answer_correctness', 'answer_relevancy', 'answer_similarity', 'average', 'average_score', 'because', 'confidence_intervals', 'context', 'context_count', 'context_precision', 'context_recall', 'contexts', 'count', 'criteria_met', 'criteria_scores', 'description', 'detailed_evaluation', 'error', 'evaluation_mode', 'excellent', 'failing', 'fair', 'faithfulness', 'flagged_issues', 'generation_quality', 'good', 'grade', 'ground_truth', 'high', 'however', 'id', 'judgments', 'low', 'max_score', 'medium', 'metadata', 'metric', 'metric_correlations', 'min_score', 'mock_mode', 'no_data', 'num_criteria', 'num_evaluations', 'num_responses', 'num_thresholds', 'num_violations', 'outlier_detection', 'overall_metrics', 'overall_performance', 'overall_score', 'pass_rate', 'passing', 'percentage', 'performance_patterns', 'performance_status', 'poor', 'poor_performers', 'query', 'question', 'report_type', 'response', 'response_id', 'response_length', 'retrieval_quality', 'sample_count', 'score', 'score_distribution', 'scores', 'severity', 'stable', 'strengths', 'summary', 'system_a_samples', 'system_b_samples', 'therefore', 'threshold', 'threshold_violations', 'tie', 'total_evaluations', 'trend', 'variable_performers', 'warning', 'weaknesses']